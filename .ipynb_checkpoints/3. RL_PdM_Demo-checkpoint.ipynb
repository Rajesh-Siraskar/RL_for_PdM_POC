{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8e900d",
   "metadata": {},
   "source": [
    "## Reinforcement Learning for Predictive Mainteance\n",
    "\n",
    "- Author: Rajesh Siraskar\n",
    "- Date: 09-Jan-2022\n",
    "\n",
    "#### Elective Objectives:\n",
    "1. Basics: RL\n",
    "2. Python basics: numpy, pandas and matplotlib\n",
    "3. Advanced: PyTorch, RL programming, TensorBoard\n",
    "4. Understand OpenAI gym and create **custom env.** \n",
    "5. Put together a RL loop - non-functional/dummy\n",
    "6. Show training curves in TensorBoard\n",
    "7. **Propose ideas for a functional model**\n",
    "\n",
    "#### Approach/Ideas:\n",
    "1. Action: Is one of No-action, Maintenance-Regular, Maintenance-Special, Replace\n",
    "2. Cost: Associate cost of taking these: Replace = 100, Special=40, Regular=20\n",
    "3. After each maintenance: Increase RUL: Replace = x10, Special=x5, Regular=x2\n",
    "4. Doing regular in downward trend of operation -> RUL increase only 10% regular\n",
    "5. Reward: Most life, least cost so for example: R = RUL/(Cost+1) i.e. as RUL high and cost low -> reward MOST \n",
    "6. What one expects: Regular maintenance done in normal cycle\n",
    "7. Optimal Policy = When to maintain with least cost\n",
    "\n",
    "- Running TensorBoard: \n",
    "    - conda activate RL_OptimalControl\n",
    "    - tensorboard --logdir E:\\Projects\\RL_for_Predictive_Maintenance\\tensorboard\\."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import json\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C, PPO, DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93808e79",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES_MAX = 4000\n",
    "\n",
    "BEARING_FAILURE_DATA_FILE = 'data\\data.csv'\n",
    "TRAINING_RESULTS_FILE = 'Test_run_A2C_09-Feb.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73177165",
   "metadata": {},
   "source": [
    "## Environment: Custom class based on OpenGym abstract class \n",
    "### Bearing failure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import json\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "MAX_LIFE = 470\n",
    "\n",
    "MAX_VIBRATION = 2\n",
    "MAX_REWARD = 10\n",
    "MAX_STEPS = 20000\n",
    "RUL_WINDOW = 10\n",
    "LAMBDA = 0.01\n",
    "INITIAL_RUL = MAX_LIFE\n",
    "\n",
    "NO_ACTION = 0\n",
    "MAINTENANCE_REGULAR = 1\n",
    "MAINTENANCE_SPECIAL = 2\n",
    "MAINTENANCE_REPLACE = 3\n",
    "\n",
    "a_cost = []\n",
    "a_action_recommended = []\n",
    "a_actions = []\n",
    "a_action_text = []\n",
    "a_rul = []\n",
    "a_rewards = []\n",
    "a_events = []\n",
    "a_time = []\n",
    "\n",
    "class BearingEnv(gym.Env):\n",
    "    \"\"\"A Bearing RUL estimating environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df):        \n",
    "        super(BearingEnv, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.maintenance_cost = 0\n",
    "        self.maintenance_events = 0\n",
    "        self.rul = 0\n",
    "        self.reward = 0\n",
    "        self.current_action = NO_ACTION\n",
    "        \n",
    "        self.reward_range = (0, MAX_REWARD)\n",
    "\n",
    "        high = np.array(\n",
    "            [\n",
    "                MAX_LIFE,\n",
    "                MAX_VIBRATION,\n",
    "                MAINTENANCE_REPLACE,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        \n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "        \n",
    "        self.state = None\n",
    "            \n",
    "    def _next_observation(self):      \n",
    "        frame = np.array([\n",
    "            self.df.loc[self.current_step, 'RUL'] / MAX_LIFE,\n",
    "            self.df.loc[self.current_step, 'vibration'] / MAX_VIBRATION,\n",
    "            self.df.loc[self.current_step, 'temperature'],\n",
    "        ])\n",
    "        \n",
    "        self.rul = self.df.loc[self.current_step: self.current_step, 'RUL'].values[0]\n",
    "        \n",
    "        # Append additional data and scale each value to between 0-1\n",
    "        obs = frame.flatten()\n",
    "        return obs\n",
    "\n",
    "    def _take_action(self, action):\n",
    "\n",
    "        if action == NO_ACTION: # Normal state\n",
    "            # 1% reduction in life\n",
    "            # self.rul *= 0.99\n",
    "            # self.maintenance_cost += 0.1\n",
    "            action_text = 'None'\n",
    "            \n",
    "        elif action == MAINTENANCE_REGULAR: \n",
    "            # 1% increase in life\n",
    "            self.rul *= 1.01\n",
    "            self.maintenance_cost += 1\n",
    "            self.maintenance_events += 1\n",
    "            action_text = 'Maintenance-Regular'\n",
    "\n",
    "        elif action == MAINTENANCE_SPECIAL: \n",
    "            # 5% increase in life\n",
    "            self.rul *= 1.05\n",
    "            self.maintenance_cost += 4\n",
    "            self.maintenance_events += 1\n",
    "            action_text = 'Maintenance-Special' \n",
    "            \n",
    "        elif action == MAINTENANCE_REPLACE: \n",
    "            # Normal state\n",
    "            self.rul = MAX_LIFE\n",
    "            self.maintenance_cost += 20\n",
    "            self.maintenance_events += 1\n",
    "            action_text = '* REPLACE *' \n",
    "            \n",
    "        #a_actions.append(action)\n",
    "        a_action_text.append(action_text)\n",
    "        self.reward = self.rul / (self.maintenance_cost+LAMBDA)\n",
    "        \n",
    "        print('{0:<20} | RUL: {1:>8.2f} | Cost: {2:>8.2f} | Maintenance events: {3:>3d} | Reward: {4:>12.3f}'.\n",
    "              format(action_text, self.rul, self.maintenance_cost, self.maintenance_events, self.reward))\n",
    "                 \n",
    "        self.state = (self.rul, self.maintenance_cost, action)\n",
    "\n",
    "  \n",
    "    def step(self, action):        \n",
    "        # Execute one time step within the environment\n",
    "        self._take_action(action)\n",
    "        reward = self.reward\n",
    "        \n",
    "        if self.current_step >= (len(self.df.loc[:, 'RUL'].values)-1):\n",
    "            done = True\n",
    "        else:\n",
    "            self.current_step += 1\n",
    "            done = False\n",
    "                \n",
    "        if self.rul <= 10:\n",
    "            done = True\n",
    "        elif self.rul >= MAX_LIFE:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        \n",
    "        a_time.append(self.current_step)\n",
    "        a_rewards.append(self.reward)\n",
    "        a_rul.append(self.rul)\n",
    "        a_cost.append(self.maintenance_cost)\n",
    "        a_actions.append(action)\n",
    "        a_events.append(self.maintenance_events)\n",
    "        \n",
    "        # From database extract recommended action\n",
    "        recommended_action = self.df.loc[self.current_step: self.current_step, 'ACTION_CODE'].values[0]\n",
    "        a_action_recommended.append(recommended_action)\n",
    "        \n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        done = False\n",
    "        self.state = np.random.uniform(low=-0.0, high=1.0, size=(3,))\n",
    "\n",
    "        # Set the current step to a random point within the data frame\n",
    "        self.current_step = 0\n",
    "        # self.current_step = random.randint(0, len(self.df.loc[:, 'RUL'].values))\n",
    "\n",
    "        self.rul = self.df.loc[self.current_step: self.current_step, 'RUL'].values[0]\n",
    "        print('\\n --- RESET. Starting RUL = ', self.rul)\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.maintenance_cost = 0\n",
    "        self.maintenance_events = 0\n",
    "        self.reward = 0\n",
    "        self.current_action = NO_ACTION\n",
    "        \n",
    "        return np.array(self.state, dtype=np.float32)\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        # Render the environment to the screen\n",
    "        RUL = self.rul\n",
    "        \n",
    "        print('>> {0:<20} | RUL: {1:>8.2f} | Cost: {2:>8.2f} | Reward: {3:>12.3f}'.\n",
    "              format(self.current_action, self.rul, self.maintenance_cost, self.reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c64be",
   "metadata": {},
   "source": [
    "### Check environment definition stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.env_checker import check_env\n",
    "# env = BearingEnv(df)\n",
    "# check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663555c7",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BEARING_FAILURE_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = EPISODES_MAX\n",
    "RUN_TEST = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f533cb",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithms require a vectorized environment to run\n",
    "env = DummyVecEnv([lambda: BearingEnv(df)])\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=\"./tensorboard/\")\n",
    "#model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=\"./tensorboard/\")\n",
    "#model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=\"./tensorboard/\")\n",
    "\n",
    "model.learn(total_timesteps=EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c6711",
   "metadata": {},
   "source": [
    "### Store results in a Pandas data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4468331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({'Time-step':a_time[:], 'Recommended_Action':a_action_recommended[:], 'Action':a_action_text[:], \n",
    "                           'Action_Code':a_actions[:], 'Rewards':a_rewards[:], 'RUL':a_rul[:], \n",
    "                           'MaintenanceEvents':a_events[:], 'Cost':a_cost[:]})\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355fbb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.Rewards = df_results.Rewards/df_results.Rewards.max()\n",
    "df_results.RUL = df_results.RUL/df_results.RUL.max()\n",
    "df_results.Cost = df_results.Cost/df_results.Cost.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bcfcf7",
   "metadata": {},
   "source": [
    "### Convert to CSV for Analysis notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d2a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(TRAINING_RESULTS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af2767f",
   "metadata": {},
   "source": [
    "### Test agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbd4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for i in range(20):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_OptimalControl",
   "language": "python",
   "name": "rl_optimalcontrol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
